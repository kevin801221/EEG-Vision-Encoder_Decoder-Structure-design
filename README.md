# EEG-Vision-Encoder_Decoder-Structure-design
![image](https://github.com/user-attachments/assets/476eb377-45b4-4f25-b553-af4b8ae4e47c)

![image](https://github.com/user-attachments/assets/726d9b9f-1d17-488e-ab13-f1efb5383618)

# EEG è¦–è¦ºè§£ç¢¼èˆ‡é‡å»ºæ¡†æ¶

ä¸€å€‹åŸºæ–¼è…¦é›»åœ–ï¼ˆEEGï¼‰çš„ç«¯åˆ°ç«¯è¦–è¦ºé‡å»ºé›¶æ¨£æœ¬æ¡†æ¶ï¼Œä½¿ç”¨è‡ªé©æ‡‰æ€ç¶­æ˜ å°„å™¨ï¼ˆATMï¼‰å°‡ç¥ç¶“ä¿¡è™Ÿè½‰æ›ç‚ºè¦–è¦ºé‡å»ºã€‚

[![Paper](https://img.shields.io/badge/Paper-arXiv-red)](arxiv_link)
[![Conference](https://img.shields.io/badge/Conference-NeurIPS%202024-blue)](neurips_link)
[![License](https://img.shields.io/badge/License-MIT-green)](license_link)

## ç›®éŒ„
- [æ¡†æ¶æ¦‚è¿°](#æ¡†æ¶æ¦‚è¿°)
- [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
- [è©³ç´°æ­¥é©Ÿ](#è©³ç´°æ­¥é©Ÿ)
- [çµæœè©•ä¼°](#çµæœè©•ä¼°)
- [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)

## æ¡†æ¶æ¦‚è¿°

æœ¬æ¡†æ¶å°‡ EEG è¦–è¦ºé‡å»ºåˆ†ç‚ºå…©å€‹é—œéµéšæ®µï¼š
1. ç‰¹å¾µæå–éšæ®µï¼šä½¿ç”¨ ATM æå–å¤šå±¤æ¬¡ç‰¹å¾µ
2. åœ–åƒç”Ÿæˆéšæ®µï¼šæ•´åˆç‰¹å¾µé‡å»ºæœ€çµ‚åœ–åƒ

### ä¸»è¦ç‰¹é»
- ğŸ§  è‡ªé©æ‡‰æ€ç¶­æ˜ å°„æŠ€è¡“
- ğŸ¯ å¤šè·¯å¾‘ä¸¦è¡Œè™•ç†
- ğŸ“ˆ ç‰¹å¾µèåˆç”Ÿæˆ
- ğŸ”„ é›¶æ¨£æœ¬é·ç§»èƒ½åŠ›

### æŠ€è¡“å„ªå‹¢
- ä½æˆæœ¬å¯¦ç¾
- é«˜æ™‚é–“åˆ†è¾¨ç‡
- å„ªç§€çš„æ³›åŒ–æ€§èƒ½
- å¤šæ¨¡æ…‹æ”¯æŒ

## å¿«é€Ÿé–‹å§‹

### 1. ç’°å¢ƒé…ç½®
```bash
# å…‹éš†å°ˆæ¡ˆ
git clone https://github.com/{username}/EEG_Image_decode.git
cd EEG_Image_decode

# é…ç½®ç’°å¢ƒ
conda env create -f environment.yml
conda activate BCI

# å®‰è£ä¾è³´
pip install -r requirements.txt
```

### 2. æ•¸æ“šæº–å‚™
```bash
# å‰µå»ºå¿…è¦ç›®éŒ„
mkdir -p data/{raw,preprocessed,output}

# ä¸‹è¼‰ç¤ºä¾‹æ•¸æ“šï¼ˆå¦‚æœéœ€è¦ï¼‰
python scripts/download_data.py
```

## è©³ç´°æ­¥é©Ÿ

### ç¬¬ä¸€éšæ®µï¼šç‰¹å¾µæå–

#### 1. EEG é è™•ç†
```bash
# é‹è¡Œé è™•ç†è…³æœ¬
python preprocessing/eeg_preprocess.py \
    --input_dir data/raw \
    --output_dir data/preprocessed \
    --sampling_rate 1000
```

#### 2. ATM ç‰¹å¾µæå–
```bash
# é‹è¡Œ ATM æå–ä¸‰è·¯ç‰¹å¾µ
python feature_extraction/run_atm.py \
    --input_data data/preprocessed \
    --output_dir data/features \
    --batch_size 32 \
    --gpu cuda:0
```

é€™ä¸€æ­¥æœƒç”¢ç”Ÿä¸‰ç¨®ç‰¹å¾µï¼š
- CLIP ç‰¹å¾µï¼ˆé«˜ç´šèªç¾©ï¼‰
- æ“´æ•£å…ˆé©—ï¼ˆçµæ§‹ä¿¡æ¯ï¼‰
- VAE ç‰¹å¾µï¼ˆè¦–è¦ºç´°ç¯€ï¼‰

### ç¬¬äºŒéšæ®µï¼šåœ–åƒé‡å»º

#### 1. ç‰¹å¾µæ•´åˆ
```bash
# æ•´åˆç¬¬ä¸€éšæ®µçš„ç‰¹å¾µ
python generation/integrate_features.py \
    --clip_features data/features/clip \
    --prior_features data/features/prior \
    --vae_features data/features/vae \
    --output_dir data/features/integrated
```

#### 2. åœ–åƒç”Ÿæˆ
```bash
# ä½¿ç”¨æ•´åˆç‰¹å¾µç”Ÿæˆæœ€çµ‚åœ–åƒ
python generation/generate_images.py \
    --input_features data/features/integrated \
    --model_type sdxl \
    --output_dir data/output/final
```

## æ¨¡å‹çµæ§‹

### ATM ç·¨ç¢¼å™¨
```python
class ATMEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.eeg_encoder = EEGEncoder()
        self.clip_projector = CLIPProjector()
        self.vae_encoder = VAEEncoder()
        self.diffusion = DiffusionModel()
```

### ç‰¹å¾µæ•´åˆå™¨
```python
class FeatureIntegrator(nn.Module):
    def __init__(self):
        super().__init__()
        self.semantic_head = SemanticHead()
        self.structure_head = StructureHead()
        self.detail_head = DetailHead()
```

## æ•¸æ“šæµç¨‹èªªæ˜

### ç¬¬ä¸€éšæ®µåˆ°ç¬¬äºŒéšæ®µçš„æ•¸æ“šæµå‹•ï¼š

1. **CLIP è·¯å¾‘**
   - è¼¸å…¥ï¼šEEG ä¿¡è™Ÿ
   - è™•ç†ï¼šATM â†’ CLIP æŠ•å½±
   - è¼¸å‡ºï¼šé«˜ç´šèªç¾©ç‰¹å¾µ
   - ç”¨é€”ï¼šæŒ‡å°åœ–åƒå…§å®¹ç”Ÿæˆ

2. **æ“´æ•£è·¯å¾‘**
   - è¼¸å…¥ï¼šEEG ä¿¡è™Ÿ
   - è™•ç†ï¼šATM â†’ æ“´æ•£æ¨¡å‹
   - è¼¸å‡ºï¼šåœ–åƒå…ˆé©—
   - ç”¨é€”ï¼šæä¾›çµæ§‹ç´„æŸ

3. **VAE è·¯å¾‘**
   - è¼¸å…¥ï¼šEEG ä¿¡è™Ÿ
   - è™•ç†ï¼šATM â†’ VAE
   - è¼¸å‡ºï¼šæ¨¡ç³Šåœ–åƒ
   - ç”¨é€”ï¼šæä¾›è¦–è¦ºç´°ç¯€

### ç‰¹å¾µæ•´åˆéç¨‹
```python
def integrate_features(semantic, structure, detail):
    # ç‰¹å¾µå°é½Š
    aligned_features = align_features(semantic, structure, detail)
    
    # ç‰¹å¾µèåˆ
    fused_features = feature_fusion(aligned_features)
    
    return fused_features
```

## åƒæ•¸é…ç½®

### ç¬¬ä¸€éšæ®µåƒæ•¸
```yaml
# config/stage1_config.yaml
atm:
  input_channels: 64
  sampling_rate: 1000
  feature_dim: 512

clip:
  model_type: "ViT-L/14"
  projection_dim: 768

vae:
  latent_dim: 512
  recon_weight: 1.0
```

### ç¬¬äºŒéšæ®µåƒæ•¸
```yaml
# config/stage2_config.yaml
diffusion:
  model: "sdxl"
  steps: 50
  guidance_scale: 7.5

integration:
  semantic_weight: 1.0
  structure_weight: 0.8
  detail_weight: 0.5
```

## çµæœè©•ä¼°

### 1. è©•ä¼°æŒ‡æ¨™
```bash
# é‹è¡Œè©•ä¼°è…³æœ¬
python evaluate/run_metrics.py \
    --generated_images data/output/final \
    --ground_truth data/test/images \
    --output_dir data/evaluation
```

### 2. å¯è¦–åŒ–çµæœ
```bash
# ç”Ÿæˆå¯è¦–åŒ–å ±å‘Š
python visualize/create_report.py \
    --results_dir data/evaluation \
    --output_path reports/evaluation.html
```

## å¸¸è¦‹å•é¡Œ

### 1. ç‰¹å¾µæå–å•é¡Œ
Q: ä¸‰å€‹ç‰¹å¾µè·¯å¾‘æ˜¯å¦å¿…é ˆåŒæ™‚ä½¿ç”¨ï¼Ÿ
A: ä¸æ˜¯å¿…é ˆçš„ï¼Œä½†å®Œæ•´ä½¿ç”¨ä¸‰å€‹è·¯å¾‘èƒ½ç²å¾—æœ€ä½³æ•ˆæœã€‚æ¯å€‹è·¯å¾‘è² è²¬ä¸åŒå±¤é¢çš„è¦–è¦ºé‡å»ºã€‚

### 2. æ•´åˆå•é¡Œ
Q: ç‰¹å¾µæ•´åˆå¤±æ•—æ€éº¼è¾¦ï¼Ÿ
A: æª¢æŸ¥ä»¥ä¸‹å¹¾é»ï¼š
- ç‰¹å¾µç¶­åº¦æ˜¯å¦åŒ¹é…
- ç‰¹å¾µç¯„åœæ˜¯å¦æ­£ç¢ºæ­¸ä¸€åŒ–
- æ¬Šé‡é…ç½®æ˜¯å¦åˆç†

### 3. æ€§èƒ½å„ªåŒ–
Q: å¦‚ä½•æå‡é‡å»ºè³ªé‡ï¼Ÿ
A: å¯ä»¥ï¼š
- èª¿æ•´ç‰¹å¾µæ¬Šé‡
- å¢åŠ è¨“ç·´æ•¸æ“š
- å„ªåŒ–ç‰¹å¾µæå–åƒæ•¸

## å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨äº†æœ¬é …ç›®çš„ä»£ç¢¼æˆ–æ–¹æ³•ï¼Œè«‹å¼•ç”¨æˆ‘å€‘çš„è«–æ–‡ï¼š
```bibtex
@article{li2024visual,
  title={Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion},
  author={Li, Dongyang and Wei, Chen and Li, Shiying and Zou, Jiachen and Liu, Quanying},
  journal={arXiv preprint arXiv:2403.07721},
  year={2024}
}
```

## è¨±å¯è­‰

æœ¬é …ç›®åŸºæ–¼ MIT è¨±å¯è­‰é–‹æºã€‚

## ç¶­è­·è€…

- æŠ€è¡“æ”¯æŒï¼š[å§“å](mailto:email@example.com)

## æ›´æ–°æ—¥èªŒ
- [2024/09/26] è«–æ–‡è¢« NeurIPS 2024 æ¥æ”¶
- [2024/09/25] æ›´æ–° arXiv è«–æ–‡
- [2024/08/01] æ›´æ–°è¨“ç·´å’Œæ¨ç†è…³æœ¬
- [2024/05/19] æ›´æ–°æ•¸æ“šé›†åŠ è¼‰è…³æœ¬
- [2024/03/12] ç™¼å¸ƒ arXiv è«–æ–‡
